{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSA (Latent Semantic Analysis(潜在意味解析))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 自然言語(人語)処理の一種"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### メモ<br>\n",
    "・多量の文章から類似度のあるものを自動的に学習してくれる。<br>\n",
    "    車と自動車は人には同類語と判別出来るが、情報検索の分野にてそれは困難  <br>\n",
    "    →車で検索をかけても自動車が抜き出されることはない<br>\n",
    "    →そこで大量の文章から自動的に単語同士の類似度、単語と文書の類似度を計算してくれるのがLSA!!<br>\n",
    "\n",
    "・LSAにより次元削減をして、潜在的な意味空間へ変換する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "・複数の文書データがあったとする。ここで４つの文書データを8種類の単語に分類できたとして\n",
    "８行４列の行列Xが出来たとする。このXは次で表すことができる。<br>\n",
    "### X = UDV\\^T= <br>\n",
    "$\\begin{bmatrix}\n",
    "0 & -1 & -1 &-1 \\\\\\ \n",
    "0 & -1 & -1 & 0 \\\\\\  \n",
    "… & …　& … &　… \\\\\\\n",
    "-1 & 0 & 0 &-1 \n",
    "\\end{bmatrix} $\n",
    "$\\begin{bmatrix}\n",
    "4 & 0 & 0 & 0  \\\\\\ \n",
    "0 & 3 & 0 & 0 \\\\\\  \n",
    "0 & 0 & 2 & 0 \\\\\\  \n",
    "0 & 0 & 0 &1 \n",
    "\\end{bmatrix}$\n",
    "$\\begin{bmatrix}\n",
    "0 & 0 & -1 & -1  \\\\\\ \n",
    "-1 & -1 & 0 & 0 \\\\\\  \n",
    "-1 & 1 & 0 & 0 \\\\\\  \n",
    "0 & 0 & 1 &1 \n",
    "\\end{bmatrix}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ・U : 単語と要約された特徴量の変換情報を持つ行列\n",
    "#### ・D : 行列の重要度を持つ行列(行列の重要度が大きい順に並ぶ)\n",
    "#### ・V : 要約された特徴量と文書の変換情報を持つ行列"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 次元削減 \n",
    "\n",
    "・次にこれを次元削減したい・・・！ \n",
    "とすると、例えば2個の特徴量に次元削減したいとするならば \\\n",
    "・・・U : ３、４列目を削除する \\\n",
    "・・・D : ２行２列にする \\\n",
    "・・・V : ３行目以降は全て削除 \\\n",
    "\n",
    "上記の手順を踏まえるとこうなる \n",
    "\n",
    "### X = UDV\\^T= <br>\n",
    "$\\begin{bmatrix}\n",
    "0 & -1  \\\\\\ \n",
    "0 & -1  \\\\\\  \n",
    "… & …　 \\\\\\\n",
    "-1 & 0 \n",
    "\\end{bmatrix} $\n",
    "$\\begin{bmatrix}\n",
    "4 & 0 \\\\\\ \n",
    "0 & 3 \n",
    "\\end{bmatrix}$\n",
    "$\\begin{bmatrix}\n",
    "0 & 0 & -1 & -1  \\\\\\ \n",
    "-1 & -1 & 0 & 0 \n",
    "\\end{bmatrix}$\n",
    "\n",
    "---\n",
    "\n",
    "### 解説\n",
    "\n",
    "・これによりXは次元削減しながらも元の行列Xの近似を取ることができた \\\n",
    "・特にUDは要約された特徴量の中から重要度の高い特徴量を選択できたことになる\n",
    "\n",
    "・次に要約された特徴量A、Bの潜在的な意味を考えることにする、下記にUDの計算ソースを記載する\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [UD変換後のデータ] \n",
      "[[ 0.00000000e+00  8.50650808e-01]\n",
      " [ 0.00000000e+00  8.50650808e-01]\n",
      " [-1.35973996e-15  1.37638192e+00]\n",
      " [-1.35973996e-15  5.25731112e-01]\n",
      " [ 1.41421356e+00  1.01096131e-15]\n",
      " [ 7.07106781e-01  6.06576787e-16]\n",
      " [ 1.41421356e+00  1.01096131e-15]\n",
      " [ 7.07106781e-01  4.04384525e-16]]\n",
      "----------\n",
      " [寄与率] \n",
      "[0.38596491 0.27999429]\n",
      "----------\n",
      " [累積寄与率] \n",
      "0.6659592065833306\n"
     ]
    }
   ],
   "source": [
    "#decomposition：分解、Truncated：切り捨てられた\n",
    "#特異値分解:SVD (singular value decomposition)\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "data = [[1, 0, 0, 0],\n",
    "[1, 0, 0, 0],\n",
    "[1, 1, 0, 0],\n",
    "[0, 1, 0, 0],\n",
    "[0, 0, 1, 1],\n",
    "[0, 0, 1, 0],\n",
    "[0, 0, 1, 1],\n",
    "[0, 0, 0, 1]]\n",
    "n_components = 2 # 潜在変数の数\n",
    "model = TruncatedSVD(n_components=n_components)\n",
    "model.fit(data)\n",
    "print(\" [UD変換後のデータ] \")\n",
    "print(model.transform(data)) # 変換したデータ \n",
    "print(\"----------\")\n",
    "print(\" [寄与率] \")\n",
    "print(model.explained_variance_ratio_) # 寄与率 \n",
    "print(\"----------\")\n",
    "print(\" [累積寄与率] \")\n",
    "print(sum(model.explained_variance_ratio_)) # 累積寄与率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### LSAの注意点\n",
    "・変換後の行列の解釈が難しい \\\n",
    "・NMF、LDAの方が解釈が容易であることが多い \\\n",
    "・文章の単語が行列の数になるため非常に大きい計算数になりコストが膨大＆計算が多いのでモデルの更新が難しい"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
